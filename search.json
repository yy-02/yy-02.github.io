[{"title":"My First Month at NCSU","url":"/post/f9a93c8d.html","content":"I've been at NCSU for a whole month now. This is my second time in America, but the first time I have stayed in America for such a long time of 4 months.\n\nI still feel that this is a dream-like and somewhat surreal experience, after all, before this I never thought I could be an exchange student abroad. Last summer when the application was open, the epidemic policy in China mainland was still strict; in addition, I had no confidence in my grades and English abilities. But I still applied in the end, and now I feel that this is one of the best decisions I have ever made in my life.\n<!--more-->\nI've been at NCSU for a whole month now. This is my second time in America, but the first time I have stayed in America for such a long time of 4 months.\n\nI still feel that this is a dream-like and somewhat surreal experience, after all, before this I never thought I could be an exchange student abroad. Last summer when the application was open, the epidemic policy in China mainland was still strict; in addition, I had no confidence in my grades and English abilities. But I still applied in the end, and now I feel that this is one of the best decisions I have ever made in my life.\n\nHuman adaptability is very strong. Within a week, I was basically familiar with the pattern of classes, studies and housing. My English is also more adequate than I thought, and I have no problem listening and reading in class, but my speaking skills are still lacking. And I find that what I do is really a great way to save money and satisfy my own tastes, and by going to Foodlion once a week, I can make meals that are both cheap and delicious (for myself).\n\n![Some dishes](https://i.ibb.co/2YHqTr8/Meal-1.png)\n\nAs far as the college itself is concerned, the campus facilities are very luxurious, far surpassing those of SJTU. I especially like the Hunt Library, which is open 24 hours a day most days. The display area on the fourth floor has become half of my workstation. The learning experience is excellent, both on the historic main campus and the modern Centennial Campus. There are many resources available for NCSU students.\n\nNCSU is also much less academically stressful than SJTU, and classes are shorter, so I have plenty of time to do what I want to do. Wolf Ridge is close to my classroom, but the Centennial campus doesn't have much food and the gym has limited facilities, so if I have free time, I still go to the main campus to find more places to play.\n\nMy ISE department is supposedly ranked in the top 15 in the US. In fact, it is much larger and more diverse than the IE department at SJTU. The IE department at SJTU is part of the School of ME and focuses on advanced manufacturing and other mechanical fields. The ISE department here, on the other hand, covers not only ISE+ME, but also ISE+Bio, with very specialized areas such as data analytics, supply chain, and even a separate ergonomics center.\n\nIn a nutshell, I really enjoy studying here. Let me end this article with some pictures I took.\n\n![Wolf Ridge](https://shuiyuan.sjtu.edu.cn/uploads/default/original/3X/d/7/d72666f4ba5a642d11afa4bcca5e0cd470141f68.jpeg)\n![Also Wolf Ridge](https://shuiyuan.sjtu.edu.cn/uploads/default/original/3X/1/b/1bde9bd07ec637d24a4bfd6618f7ed02f8bd5e9b.jpeg)\n![Main Campus](https://shuiyuan.sjtu.edu.cn/uploads/default/original/3X/9/7/9751ebbbd0902d098511d03c85e5553b635d1c27.jpeg)\n![Beautiful Clouds](https://shuiyuan.sjtu.edu.cn/uploads/default/original/3X/8/5/85fa37c99d55ab4cd062106dba46cab8a62aaaa4.jpeg)\n![Hunt Library](https://shuiyuan.sjtu.edu.cn/uploads/default/original/3X/d/d/dd952aa93d4ca93729f8ca947c273e238fbf0fec.jpeg)\n![4th Floor](https://shuiyuan.sjtu.edu.cn/uploads/default/original/3X/0/1/01550efba9e010d56b50d1682a9c44f9d27a550f.jpeg)\n![Wolf Ridge, but blue sky](https://shuiyuan.sjtu.edu.cn/uploads/default/optimized/3X/0/9/0959d1f18622a3a44cb1ddc2003b8f6c118fdf47_2_1035x775.jpeg)\n![The Oval](https://shuiyuan.sjtu.edu.cn/uploads/default/original/3X/2/f/2f14817d87725ae862459b17eb082a1295f1d4c0.jpeg)\n![Also the oval](https://shuiyuan.sjtu.edu.cn/uploads/default/original/3X/7/1/71a50d72594fc2b9257bb906d4a750aa23bb38b3.jpeg)","tags":["NCSU"],"categories":["NCSU exchange life"]},{"title":"Welcome to my personal blog!","url":"/post/cda5e84f.html","content":"This is YY's personal blog.\n\n\nVisit [About](https://yy-02.github.io/about/) to know more about me.\n\nThank you :)\n\n\n"},{"title":"An article about DDPG&PI Controllers","url":"/post/6f503a8d.html","content":"\nIn article [*__On the Combination of PID control and Reinforcement Learning: A Case Study with Water Tank System__*](https://doi.org/10.1109/ICIEA51954.2021.9516140), DDPG is used to help PI controllers to control system (in this article the target is  maintaining the water level).\nI will just paraphrase DDPG that written in article. But I think another blog should be written to fully explain DDPG since it contains many other algorithm about RL (On condition that I have completely understood DDPG).\n<!--more-->\n# The Model of Two-level Water Tank System\n![imagine](https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9516033/9516034/9516140/9516140-fig-1-source-large.gif)\nIn picture above shows the basic structure of Two-level water tank system. For the $l$-th tank, the model is\n$$h_l(t)=\\frac{F_{inl}(t)-F_{outl}(t)}{A}$$\nwhere $h_l$ is the height level of water, $A$ is the cross-sectional area of the tank, $F_{inl}$ and $F_{outl}$ represent the quantity of input and output water flow, respectively. And the whole system is gravity drained as their output water flow speeds are related to their own height of water level. Actually we can use second-order characteristics to solve this problem, but in RL training we just regard it as a black-box model.\n# Algorithm\nThe authors used DDPG with traditional PI controllers to maintain the water level, since water tank system is a continuous action space. DDPG (Deep Deterministic Policy Gradient) is a model-free off-policy Actor-Critic algorithm, combining DPG (Deterministic Policy) with DQN (Deep Q-Network). A random process $N$ is added to policy function to encourage agent to explore. \n\nAt each step, the actor takes action $a_t$ based on pre-action state $s_t$ under current policy $\\mu_t$. The reward $r_t$ is calculated based on the post-action state $s_{t+1}$. The $(s_t,a_t,r_t,s_{t+1})$ transients will be stored into the replay buffer $R$ for training two networks. A random mini-batch of $N$ transitions $(s_i,a_i,r_i,s_{i+1})$ will be sampled from replay buffer $R$ to train the critic agent through minimizing the following loss:\n$$L=\\frac{\\underset{i}{\\sum}(y_i-Q(s_i,a_i|\\theta ^Q))^2}{N}$$\nwhere $y_i$ is the sum of the reward $r_i$ and $\\gamma$-discounted expected long-term reward under current policy starting from the next time step:\n$$y_i=r_i+\\gamma Q'(s_{i+1},\\mu'(s_{i+1}|\\theta^{\\mu '})|\\theta ^{Q'})$$\nAnd in the actor agent the policy will be updated by gradient as follows:\n$$\\nabla_{\\theta\\mu}J\\approx\\frac{\\underset{i}{\\sum}\\nabla_aQ(s,a|\\theta^Q)|_{s=s_i,a=\\mu(s_i)}\\nabla_{\\theta\\mu}\\mu(s|\\theta^\\mu)|_{s_i}}{N}$$ \nFor actor agent its parameter needs to use gradient ascent while critic agent needs to minimizing its loss function with gradient descent.\n$$\\theta^\\mu=\\theta^\\mu+\\alpha\\nabla_{\\theta\\mu}J$$\n$$\\theta^Q=\\theta^Q-\\beta\\nabla_{\\theta Q}L$$\nwhere $\\alpha,\\beta$ are  learning rates. At the end of each episode, the target networks will be soft-updated as follows:\n$$\\theta^{\\mu'}\\leftarrow\\tau\\theta^\\mu+(1-\\tau)\\theta^{\\mu'}$$\n$$\\theta^{Q'}\\leftarrow\\tau\\theta^Q+(1-\\tau)\\theta^{Q'}$$\nwhere $\\tau$ is the learning target smooth factor. This is  an important difference between DQN and DDPG as the target network will be fixed during each episode in DQN, but in DDPG it will be updated dynamically, though very slowly (smoothly) due to $\\tau\\ll1$.\n\nIn water tank system, the pseudocode of DDPG is as follows:\n\n![image|690x882](https://shuiyuan.sjtu.edu.cn/uploads/default/original/3X/6/6/669b3f0b8c7706c7eebb8cb757200bf4a3b1e3b1.jpeg)\n\nThe state is defined as:\n$$state=\\bigg [\\int(l_d-l_c)dt, l_d-l_c, l_c\\bigg]$$\n\nAnd the reward function is defined as:\n$$ reward=\\begin{cases}\n-|l_d-l_c| & 0\\leq l_c\\leq4 \\\\\n-500 & else \\\\\n\\end{cases}$$ \n# Simulation Results\n## DDPG or PI ONLY\n![image](https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9516033/9516034/9516140/9516140-fig-3-source-small.gif)\n\nCompared with single PI control, the transient response speed is improved by DDPG agent, and the largest overshoot amplitude is only 12.6%. But we can see that in the steady state, there exists a clear bias in single DDPG control result.\n## DDPG+PI\n\n\n![image](https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9516033/9516034/9516140/9516140-fig-5-source-large.gif)\n\nWith the guidance of PI controller, the training process of DDPG agent is accelerated. The overshoot amplitude is only half of the amplitude in solo PI control result. The bias of solo DDPG control also disappeared and the water level converges exactly to the desired value, which means the combination of them has advantages of both PI and DDPG.\n## DDPG&PI (With Noise)\n![image](https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9516033/9516034/9516140/9516140-fig-6-source-large.gif)\n\nFor solo PI control, the impact of noise is mainly showed in the oscillation state before exactly converging to the desire level. On the contrary, the noise affects more on the transient performance in solo DDPG control, and the bias still exists.\n\nFor DDPG+PI control, the noise have little influence on the whole control process, the combination has strong anti-disturbance ability. Even under noise the DDPG+PI agent's performance is still better than single PI or DDPG control without noise.\n## Changing Desired Water Level\n![image](https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9516033/9516034/9516140/9516140-fig-7-source-large.gif)\n\nFrom my perspective, this result is the most interesting part in simulation results. First they switched the target value from 1.5$m$ to 2.5$m$, then after about 1000 seconds let the target value return to 1.5$m$. As the article said, at the first stage the PI+DDPG agent chose to follow PI controller, and we can see that they have an almost same overshoot amplitude in the figure. But at the final stage the PI+DDPG agent chose to follow DDPG control and shows an improved control performance compared with single PI control.\n# Some of my opinions\nThe authors' explanation for the phenomenon above is that the DDPG+PI agent had learned in the first stage when in the final stage, so it will perform better. Since the authors haven't provide their source code about this article yet, we are not able to know the details of their simulation. If the result is true, why the DDPG+PI agent will choose to follow PI controller at first? In my opinion, in the first stage when the system is switched from a steady state to another state, the agent's response should be the same as what we have seen in DDPG+PI control (without noise) above, since the reward function doesn't pay attention to the absolute value but the relative value of water level.","tags":["DDPG","PI","Water tank system"],"categories":["RL-Articles"]},{"title":"An Article about Double Q-PID Algorithm","url":"/post/1ab1d1e9.html","content":"\nIn article [*Double $Q$-PID algorithm for mobile robot control*](https://doi.org/10.1016/j.eswa.2019.06.066), the authors using Double $Q$-PID algorithm in PID controllers, which is similar with DQN and DDQN.\n<!--more-->\n# Knowledge before reading this article\n## $Q$-learning\nReinforcement learning involves an agent, a set of states *__S__*, and a set *__A__* of actions per state. By performing an action *__a__* $\\in$ *__A__* , the agent transitions from state to state. Executing an action in a specific state provides the agent with a reward (a numerical score).\n  \nWe define function $Q$ to calculates the quality of a state-action combination. The core of $Q$-learning is *Bellman Equation*, which is used to update $Q$-Function in value iteration. \n$$Q^{new}(s_t,a_t)\\leftarrow \\underset{old\\ value}{\\underbrace{Q(s_t,a_t)}}+\\underset{learning\\ rate}{\\underbrace{\\alpha}}\\overset{temporal\\ difference}{\\overbrace{[\\underset{new\\ value(temporal\\ difference\\ target)}{\\underbrace{\\underset{reward}{\\underbrace{r_{t+1}}}+\\underset{discount\\ factor}{\\underbrace{\\gamma}}\n{\\underset{estimate\\ of\\ optimal\\ future\\ value}{\\underbrace{\\underset{a}{max}Q_B(s_{t+1},a_{t+1})}}}}} -\\underset{old\\ value}{\\underbrace{Q(s_t,a_t)}}]}}$$\n\n## DQN & DDQN\n### DQN\nDQN is an algorithm that combine deep learning with $Q$-learning. In traditional $Q$-learning, we usually use a matrix to save the value of $Q$ for each state-action combination(Or using function approximation in large questions). \n\nIn DQN we have :\n\n- A $Q$-estimation network\n- A $Q$-target network\n- An experience pool  \n\nIn a learning period, the weight of $Q$-target network is fixed and $Q$-estimation network is updated by the value of state-action combination in $Q$-target network. At the end of this period, the weight of $Q$-target network is updated by $Q$-estimation network.\n\n__Disadvantage :__ In DQN the value of $Q$ will be overestimated, because $Q$-esitimation network is updated by $Q$-target network.\n### DDQN\nTo solve the over estimation problem above, DDQN is proposed. The difference between DQN and DDQN is that, in DDQN action is selected by its value in $Q$-estimation network and then $Q$-target network will use this action to update the value of $Q$. \n# About this article\n## Double Q-learning\nThis algorithm is very similar with DQN and DDQN. It also includes two neural networks, but they will be updated by each other. For example, if the policy uses the function $Q_A$ to choose the action the optimal action, then $Q_A$ will be updated as:\n  \n  $$Q_A(s_t,a_t)\\leftarrow Q_A(s_t,a_t)+\\alpha[r_{t+1}+\\gamma\\underset{a}{max} Q_B(s_{t+1},a_{t+1})-Q_A(s_t,a_t)]$$\n  Alternatively, if the policy uses the function $Q_B$, then it will be updated as:\n  $$Q_B(s_t,a_t)\\leftarrow Q_B(s_t,a_t)+\\alpha[r_{t+1}+\\gamma\\underset{a}{max} Q_A(s_{t+1},a_{t+1})-Q_B(s_t,a_t)]$$\n  As the article says, \"Since the update of $Q_A$ and $Q_B$ is done with its opposite, each update takes into consideration a different subset of the data set, thus removing the bias in the value estimation.\"\n  ## Incremental discretization of the states and actions space\n  In this article they presented the main ideas of incremental discretization of the state space and action space developed in [Carlucho et al.(2017)](https://doi.org/10.1016/J.ESWA.2017.03.002). Briefly speaking, at first they have a fairly coarse discretization of the action space $A$, then if action $a$ is selected it will be divided into two parts (or we can say it is a splitting process). As the discretization process going on, actions which have been chosen more will have higher discretization level.\n  The final result represents the discretization of action space $A$.\n  \n  ![image](https://ars.els-cdn.com/content/image/1-s2.0-S0957417419304749-gr3_lrg.jpg)\n  ## Double $Q$-PID algorithm \n  ### Incremental active learning\n  Using discretization methods above, the agent can avoid unnecessary exploration of large regions, concentrating in a more relevant space. This method may be useful in working with autonomous mobile robots.\n\n  ![](https://ars.els-cdn.com/content/image/1-s2.0-S0957417419304749-gr4_lrg.jpg)\n  ### Algorithm statement\n  The pseudocode is as follows. $x,k$ are separately referred to as *state* and *action*.\n\n  ![](https://ars.els-cdn.com/content/image/1-s2.0-S0957417419304749-gr19_lrg.jpg)\n  ## Some meaningful results\n\n  - Double $Q$-PID algorithm had quickly learned to cancel the derivative terms, which means in the simulation PID controllers were equal to PI controllers.\n  \n  - Double $Q$-PID algorithm shows its ability in robot control, especially in balance control. It has strong robustness, as it only needs to provide an approximate initial coarse discretization of the action space. This advantage enables us to use it in different kinds of control models.","tags":["Double Q-PID","Robot control"],"categories":["RL-Articles"]}]